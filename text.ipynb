{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n-Nd7J315K3K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# from keras.utils import np_utils # type: ignore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1TBp0vIz5K3M"
      },
      "outputs": [],
      "source": [
        "filename = \"blackfriday.txt\"\n",
        "raw_text = open(filename, encoding=\"UTF-8\").read()\n",
        "raw_text = raw_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-2CWkxzr5K3N"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyvwqcat5K3N",
        "outputId": "96ec9cfe-416c-4af6-eed9-7ffa7d5c0031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Characters:  5515\n",
            "Total Vocab:  66\n"
          ]
        }
      ],
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGVV_A_15K3N",
        "outputId": "fa73de45-9bb1-4607-8910-b55dbfcc8c16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Patterns:  5415\n"
          ]
        }
      ],
      "source": [
        "seq_lenght = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_lenght, 1):\n",
        "    seq_in = raw_text[i:i + seq_lenght]\n",
        "    seq_out = raw_text[i + seq_lenght]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "blQcI6MQ5K3O"
      },
      "outputs": [],
      "source": [
        "X = np.reshape(dataX, (n_patterns, seq_lenght, 1))\n",
        "X = X / float(n_vocab)\n",
        "y = to_categorical(dataY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0SpjMh35K3O",
        "outputId": "c231b8f4-c5e8-41e6-cb49-37462f755ef0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkwkftNa5K3O",
        "outputId": "392edf47-5cfb-4a03-f7f7-95e23fb2d8bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 3.4715\n",
            "Epoch 1: loss improved from inf to 3.30783, saving model to weights-improvement-01-3.3078.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - loss: 3.4696\n",
            "Epoch 2/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 3.2452\n",
            "Epoch 2: loss improved from 3.30783 to 3.22018, saving model to weights-improvement-02-3.2202.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - loss: 3.2449\n",
            "Epoch 3/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 3.2009\n",
            "Epoch 3: loss improved from 3.22018 to 3.21347, saving model to weights-improvement-03-3.2135.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - loss: 3.2010\n",
            "Epoch 4/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 3.1965\n",
            "Epoch 4: loss improved from 3.21347 to 3.19144, saving model to weights-improvement-04-3.1914.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - loss: 3.1965\n",
            "Epoch 5/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 3.1726\n",
            "Epoch 5: loss improved from 3.19144 to 3.15592, saving model to weights-improvement-05-3.1559.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - loss: 3.1724\n",
            "Epoch 6/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 3.1009\n",
            "Epoch 6: loss improved from 3.15592 to 3.08659, saving model to weights-improvement-06-3.0866.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - loss: 3.1008\n",
            "Epoch 7/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 3.0035\n",
            "Epoch 7: loss improved from 3.08659 to 3.01161, saving model to weights-improvement-07-3.0116.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - loss: 3.0036\n",
            "Epoch 8/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 2.9459\n",
            "Epoch 8: loss improved from 3.01161 to 2.93250, saving model to weights-improvement-08-2.9325.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 1s/step - loss: 2.9457\n",
            "Epoch 9/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 2.8238\n",
            "Epoch 9: loss improved from 2.93250 to 2.78398, saving model to weights-improvement-09-2.7840.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - loss: 2.8234\n",
            "Epoch 10/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 2.6319\n",
            "Epoch 10: loss improved from 2.78398 to 2.58844, saving model to weights-improvement-10-2.5884.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 1s/step - loss: 2.6314\n",
            "Epoch 11/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 2.3646\n",
            "Epoch 11: loss improved from 2.58844 to 2.32383, saving model to weights-improvement-11-2.3238.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - loss: 2.3641\n",
            "Epoch 12/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 2.0567\n",
            "Epoch 12: loss improved from 2.32383 to 2.03975, saving model to weights-improvement-12-2.0398.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - loss: 2.0565\n",
            "Epoch 13/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 1.8438\n",
            "Epoch 13: loss improved from 2.03975 to 1.79641, saving model to weights-improvement-13-1.7964.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 1s/step - loss: 1.8433\n",
            "Epoch 14/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 1.5932\n",
            "Epoch 14: loss improved from 1.79641 to 1.56834, saving model to weights-improvement-14-1.5683.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - loss: 1.5929\n",
            "Epoch 15/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 1.3776\n",
            "Epoch 15: loss improved from 1.56834 to 1.37259, saving model to weights-improvement-15-1.3726.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 1s/step - loss: 1.3775\n",
            "Epoch 16/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 1.2234\n",
            "Epoch 16: loss improved from 1.37259 to 1.22029, saving model to weights-improvement-16-1.2203.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - loss: 1.2233\n",
            "Epoch 17/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 1.0611\n",
            "Epoch 17: loss improved from 1.22029 to 1.08962, saving model to weights-improvement-17-1.0896.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - loss: 1.0614\n",
            "Epoch 18/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 1.0089\n",
            "Epoch 18: loss improved from 1.08962 to 0.97487, saving model to weights-improvement-18-0.9749.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - loss: 1.0085\n",
            "Epoch 19/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.8695\n",
            "Epoch 19: loss improved from 0.97487 to 0.88378, saving model to weights-improvement-19-0.8838.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - loss: 0.8697\n",
            "Epoch 20/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.7890\n",
            "Epoch 20: loss improved from 0.88378 to 0.80276, saving model to weights-improvement-20-0.8028.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - loss: 0.7892\n",
            "Epoch 21/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.7239\n",
            "Epoch 21: loss improved from 0.80276 to 0.72847, saving model to weights-improvement-21-0.7285.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1s/step - loss: 0.7239\n",
            "Epoch 22/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.6744\n",
            "Epoch 22: loss improved from 0.72847 to 0.66817, saving model to weights-improvement-22-0.6682.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 1s/step - loss: 0.6743\n",
            "Epoch 23/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.5928\n",
            "Epoch 23: loss improved from 0.66817 to 0.61583, saving model to weights-improvement-23-0.6158.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 1s/step - loss: 0.5930\n",
            "Epoch 24/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.5274\n",
            "Epoch 24: loss improved from 0.61583 to 0.55468, saving model to weights-improvement-24-0.5547.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - loss: 0.5278\n",
            "Epoch 25/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.4831\n",
            "Epoch 25: loss improved from 0.55468 to 0.50102, saving model to weights-improvement-25-0.5010.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - loss: 0.4833\n",
            "Epoch 26/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.4684\n",
            "Epoch 26: loss improved from 0.50102 to 0.46294, saving model to weights-improvement-26-0.4629.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - loss: 0.4684\n",
            "Epoch 27/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.4170\n",
            "Epoch 27: loss improved from 0.46294 to 0.41778, saving model to weights-improvement-27-0.4178.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - loss: 0.4170\n",
            "Epoch 28/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.3688\n",
            "Epoch 28: loss improved from 0.41778 to 0.37048, saving model to weights-improvement-28-0.3705.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 1s/step - loss: 0.3689\n",
            "Epoch 29/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.3375\n",
            "Epoch 29: loss improved from 0.37048 to 0.33966, saving model to weights-improvement-29-0.3397.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1s/step - loss: 0.3376\n",
            "Epoch 30/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2737\n",
            "Epoch 30: loss improved from 0.33966 to 0.30233, saving model to weights-improvement-30-0.3023.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - loss: 0.2740\n",
            "Epoch 31/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2600\n",
            "Epoch 31: loss improved from 0.30233 to 0.26742, saving model to weights-improvement-31-0.2674.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 1s/step - loss: 0.2601\n",
            "Epoch 32/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2534\n",
            "Epoch 32: loss improved from 0.26742 to 0.25152, saving model to weights-improvement-32-0.2515.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - loss: 0.2533\n",
            "Epoch 33/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.2123\n",
            "Epoch 33: loss improved from 0.25152 to 0.22073, saving model to weights-improvement-33-0.2207.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - loss: 0.2124\n",
            "Epoch 34/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1966\n",
            "Epoch 34: loss improved from 0.22073 to 0.20496, saving model to weights-improvement-34-0.2050.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - loss: 0.1967\n",
            "Epoch 35/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1822\n",
            "Epoch 35: loss improved from 0.20496 to 0.18444, saving model to weights-improvement-35-0.1844.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - loss: 0.1822\n",
            "Epoch 36/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1597\n",
            "Epoch 36: loss improved from 0.18444 to 0.16958, saving model to weights-improvement-36-0.1696.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 1s/step - loss: 0.1598\n",
            "Epoch 37/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1516\n",
            "Epoch 37: loss improved from 0.16958 to 0.15369, saving model to weights-improvement-37-0.1537.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - loss: 0.1516\n",
            "Epoch 38/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997ms/step - loss: 0.1446\n",
            "Epoch 38: loss improved from 0.15369 to 0.14017, saving model to weights-improvement-38-0.1402.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 998ms/step - loss: 0.1446\n",
            "Epoch 39/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1211\n",
            "Epoch 39: loss improved from 0.14017 to 0.12970, saving model to weights-improvement-39-0.1297.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 1s/step - loss: 0.1212\n",
            "Epoch 40/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.1058\n",
            "Epoch 40: loss improved from 0.12970 to 0.11372, saving model to weights-improvement-40-0.1137.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - loss: 0.1059\n",
            "Epoch 41/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0987\n",
            "Epoch 41: loss improved from 0.11372 to 0.11161, saving model to weights-improvement-41-0.1116.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - loss: 0.0988\n",
            "Epoch 42/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0961\n",
            "Epoch 42: loss improved from 0.11161 to 0.09699, saving model to weights-improvement-42-0.0970.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1s/step - loss: 0.0961\n",
            "Epoch 43/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000ms/step - loss: 0.0887\n",
            "Epoch 43: loss improved from 0.09699 to 0.09349, saving model to weights-improvement-43-0.0935.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 1s/step - loss: 0.0888  \n",
            "Epoch 44/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0766\n",
            "Epoch 44: loss improved from 0.09349 to 0.08276, saving model to weights-improvement-44-0.0828.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 1s/step - loss: 0.0767\n",
            "Epoch 45/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0838\n",
            "Epoch 45: loss improved from 0.08276 to 0.08158, saving model to weights-improvement-45-0.0816.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - loss: 0.0838\n",
            "Epoch 46/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0678\n",
            "Epoch 46: loss improved from 0.08158 to 0.07560, saving model to weights-improvement-46-0.0756.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - loss: 0.0679\n",
            "Epoch 47/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0743\n",
            "Epoch 47: loss did not improve from 0.07560\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - loss: 0.0743\n",
            "Epoch 48/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998ms/step - loss: 0.0632\n",
            "Epoch 48: loss improved from 0.07560 to 0.06668, saving model to weights-improvement-48-0.0667.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 999ms/step - loss: 0.0633\n",
            "Epoch 49/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0605\n",
            "Epoch 49: loss improved from 0.06668 to 0.06066, saving model to weights-improvement-49-0.0607.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - loss: 0.0605\n",
            "Epoch 50/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0581\n",
            "Epoch 50: loss improved from 0.06066 to 0.05817, saving model to weights-improvement-50-0.0582.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - loss: 0.0581\n",
            "Epoch 51/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0565\n",
            "Epoch 51: loss improved from 0.05817 to 0.05679, saving model to weights-improvement-51-0.0568.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - loss: 0.0565\n",
            "Epoch 52/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0555\n",
            "Epoch 52: loss did not improve from 0.05679\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - loss: 0.0555\n",
            "Epoch 53/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0583\n",
            "Epoch 53: loss improved from 0.05679 to 0.05631, saving model to weights-improvement-53-0.0563.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - loss: 0.0583\n",
            "Epoch 54/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0543\n",
            "Epoch 54: loss improved from 0.05631 to 0.05419, saving model to weights-improvement-54-0.0542.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - loss: 0.0543\n",
            "Epoch 55/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0449\n",
            "Epoch 55: loss improved from 0.05419 to 0.04789, saving model to weights-improvement-55-0.0479.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - loss: 0.0449\n",
            "Epoch 56/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0423\n",
            "Epoch 56: loss improved from 0.04789 to 0.04368, saving model to weights-improvement-56-0.0437.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - loss: 0.0423\n",
            "Epoch 57/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0486\n",
            "Epoch 57: loss did not improve from 0.04368\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - loss: 0.0485\n",
            "Epoch 58/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0372\n",
            "Epoch 58: loss improved from 0.04368 to 0.03777, saving model to weights-improvement-58-0.0378.keras\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - loss: 0.0372\n",
            "Epoch 59/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998ms/step - loss: 0.0412\n",
            "Epoch 59: loss did not improve from 0.03777\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 999ms/step - loss: 0.0413\n",
            "Epoch 60/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0399\n",
            "Epoch 60: loss did not improve from 0.03777\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - loss: 0.0399\n",
            "Epoch 61/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0405\n",
            "Epoch 61: loss did not improve from 0.03777\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 1s/step - loss: 0.0406\n",
            "Epoch 62/80\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.0517\n",
            "Epoch 62: loss did not improve from 0.03777\n",
            "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - loss: 0.0518\n",
            "Epoch 63/80\n",
            "\u001b[1m66/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”\u001b[0m \u001b[1m19s\u001b[0m 1s/step - loss: 0.0463"
          ]
        }
      ],
      "source": [
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.keras\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
        "callbacks_list = [checkpoint]\n",
        "model.fit(X, y, epochs=80, batch_size=64, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afH93deNJb11"
      },
      "source": [
        "import sys\n",
        "\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "filename = \"weights-improvement-49-0.3748.keras\"\n",
        "model.load_weights(filename)\n",
        "start = np.random.randint(0, len(dataX) - 1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "for i in range(1000):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "887MSOZ5JR5m",
        "outputId": "b8ca5e89-a4b8-4545-973f-8ba75b79ba7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"  day of the black friday sale. 100% of surplus discounted. sale over eod and normal prices resume \n",
            "b \"\n",
            "inal call black friday sale ends in 3 days â€“ 15% off sitewide!ğŸ–¤ -  \n",
            "black friday sale i on ğŸ”¥ 50% off right now ğŸ¥µ \n",
            "the black friday sales is live !!\n",
            "everything 20 % off, with up to 40% off with the 3plus1 coupon!\n",
            "the sale ends on november 30!\n",
            "\n",
            "link is in the comment below! \n",
            "black friday 50% off sale ends midnight sunday pst ğŸ’¥ğŸ’¥ğŸ’¥  \n",
            "black friday weekend sale\n",
            "ends today 8th december\n",
            "\n",
            "20% discount off all sale items. \n",
            "\n",
            "\n",
            "\n",
            "mhhsbd \n",
            "black friday sale is liveğŸ¦‹ up to 40% off worldwide !!!\n",
            "\n",
            " \n",
            "last day of the black friday sale. 100% of surplus discounted. sale over eod and normal prices resume \n",
            "final call black friday sale ends in 3 days â€“ 15% off sitewide!ğŸ–¤ -  \n",
            "black friday sale i on ğŸ”¥ 50% off right now ğŸ¥µ \n",
            "the black friday sales is live !!\n",
            "everything 20 % off, with up to 40% off with the 3plus1 coupon!\n",
            "the sale ends on november 30!\n",
            "\n",
            "link is in the comment below! \n",
            "black friday 50% off sale ends midnight sunday pst ğŸ’¥ğŸ’¥ğŸ’¥  \n",
            "black friday weekend sale\n",
            "ends today 8th december\n",
            "\n",
            "20% discount off all sale ite\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "filename = \"weights-improvement-72-0.0274.keras\"\n",
        "model.load_weights(filename)\n",
        "\n",
        "start = np.random.randint(0, len(dataX) - 1)\n",
        "pattern = dataX[start]\n",
        "\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "# Táº¡o má»™t danh sÃ¡ch Ä‘á»ƒ lÆ°u káº¿t quáº£ dá»± Ä‘oÃ¡n\n",
        "predicted_text = []\n",
        "\n",
        "for i in range(1000):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "\n",
        "    # LÆ°u káº¿t quáº£ dá»± Ä‘oÃ¡n vÃ o danh sÃ¡ch\n",
        "    predicted_text.append(result)\n",
        "\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nDone.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMQ-6mu9H49K"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "\n",
        "# Define keys\n",
        "consumer_key = 'u8ocvdT7BWgfJPPAUf2hIxNsy'\n",
        "consumer_secret = 'oICveYHz0h4eIimhhVjerRE2g1ZiojVb75fwCI1AU7Rqt08y2J'\n",
        "# Access\n",
        "access_token = '1144097180187164673-DJ6vhx9aw6ceoLGrgGLnB5K3jzz2WS'\n",
        "access_secret = 'pW1rtGI4QsfdoiDCwC9Gkx8wLguZ0lHYfHYsHzIVzkpWj'\n",
        "\n",
        "# Create client\n",
        "client = tweepy.Client(\n",
        "    consumer_key=consumer_key,\n",
        "    consumer_secret=consumer_secret,\n",
        "    access_token=access_token,\n",
        "    access_token_secret=access_secret\n",
        ")\n",
        "\n",
        "# Ensure predicted_text is a string\n",
        "predicted_text = ''.join(predicted_text)  # Join the list of characters into a single string\n",
        "\n",
        "# Create tweet\n",
        "response = client.create_tweet(text=predicted_text)\n",
        "\n",
        "# Print response (optional, to see the result)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity: 1.0277788320849788\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "final_loss = 0.0274\n",
        "perplexity = math.exp(final_loss)\n",
        "\n",
        "print(f\"Perplexity: {perplexity}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
